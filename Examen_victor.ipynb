{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1vYL3FysFErZQR2rzAG0ow27DL0Vd-NXt",
      "authorship_tag": "ABX9TyP2YSptpRDebBfYXlnZp324",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Victor074/Examen_Aprendizaje_Profundo_VICTOR/blob/main/Examen_victor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Victor Alberto Lopez Cardona\n",
        "\n",
        "Exp. 747175"
      ],
      "metadata": {
        "id": "rOxi9hPBAnBB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUfR90BD7ioS"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/ML/m3ex02-data.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "j-vHweWX9Xx-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el data loader hago un resize para las imagenes de 256*256, y defino los labels (COVID-19, Non-COVID, Noraml) para que se puedan agregar a la data"
      ],
      "metadata": {
        "id": "___UeC000V-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#UNET for processing lung masks\n",
        "#RESNET for classifying radiography\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "def get_data_loader(batch_size=16):\n",
        "    label_mapping = {\n",
        "        'COVID-19': 0,\n",
        "        'Non-COVID': 1,\n",
        "        'Normal': 2\n",
        "    }\n",
        "    transform = {\n",
        "        'image': transforms.Compose([\n",
        "            transforms.Resize([256,256]),\n",
        "            transforms.ToTensor(),\n",
        "        ]),\n",
        "        'mask': transforms.Compose([\n",
        "            transforms.Resize([256,256],interpolation=transforms.InterpolationMode.NEAREST),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "    }\n",
        "    path = \"\"\n",
        "    train_paths = [(path+\"/content/m3ex02-data/Train/COVID-19/images\", path + \"/content/m3ex02-data/Train/COVID-19/lung masks\"),\n",
        "                   (path+\"/content/m3ex02-data/Train/Non-COVID/images\",path+\"/content/m3ex02-data/Train/Non-COVID/lung masks\"),\n",
        "                   (path+\"/content/m3ex02-data/Train/Normal/images\",path+\"/content/m3ex02-data/Train/Normal/lung masks\")]\n",
        "    test_paths = [(path+\"/content/m3ex02-data/Test/COVID-19/images\", path + \"/content/m3ex02-data/Test/COVID-19/lung masks\"),\n",
        "                  (path+\"/content/m3ex02-data/Test/Non-COVID/images\",path+\"/content/m3ex02-data/Test/Non-COVID/lung masks\"),\n",
        "                  (path+\"/content/m3ex02-data/Test/Normal/images\",path+\"/content/m3ex02-data/Test/Normal/lung masks\")]\n",
        "    val_paths = [(path+\"/content/m3ex02-data/Val/COVID-19/images\", path + \"/content/m3ex02-data/Val/COVID-19/lung masks\"),\n",
        "                 (path+\"/content/m3ex02-data/Val/Non-COVID/images\",path+\"/content/m3ex02-data/Val/Non-COVID/lung masks\"),\n",
        "                 (path+\"/content/m3ex02-data/Val/Normal/images\",path+\"/content/m3ex02-data/Val/Normal/lung masks\")]\n",
        "    all_paths = [train_paths,test_paths,val_paths]\n",
        "    dataloader = []\n",
        "    for one_path in all_paths:\n",
        "        data = []\n",
        "        for radio,mask in one_path:\n",
        "            radiotemp = os.path.split(radio)\n",
        "            label = label_mapping[os.path.basename(radiotemp[0])]\n",
        "            fsortlist = [file for file in sorted(os.listdir(radio))]\n",
        "            for imgfile in fsortlist:\n",
        "                image = transform[\"image\"](Image.open(os.path.join(radio,imgfile)))\n",
        "                imagemask = transform[\"mask\"](Image.open(os.path.join(mask,imgfile)))\n",
        "                data.append((image, imagemask, label))\n",
        "        dataloader.append(torch.utils.data.DataLoader(dataset=data, batch_size=batch_size, shuffle=True, num_workers=4))\n",
        "    return dataloader\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T-X2ueo50Vdl"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizo un batch size de 4 y num_workers de 4 para agilizar el proceso de entrenamiento"
      ],
      "metadata": {
        "id": "i4cXw1X-_YHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = get_data_loader(batch_size=20)\n",
        "#/content/m3ex02-data/Train/COVID-19/images"
      ],
      "metadata": {
        "id": "3jeshX03UWGy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el dataloader estoy procesando los 3 paths existentes para train, validation, testing, por lo que regreso una lista conteniendo toda la informacion"
      ],
      "metadata": {
        "id": "MVEUZeFy_fZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = all_data[0]\n",
        "testloader = all_data[1]\n",
        "valloader = all_data[2]"
      ],
      "metadata": {
        "id": "a98wy_5_d37F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este modelo estoy usando una resnet18 para que sea mas sencillo de entrenar y modifico la primera capa para que pueda recibir un canalya que la imagen de entrada esta en escala de grises, y despues conecto una UNeT pre entrenada donde tambien cambie la entrada de la primera capa para que pudiera ser compatible, y al final agregue un dropout para prevenir overfitting."
      ],
      "metadata": {
        "id": "Wwf4uZuI_sje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class myunet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, init_features=32):\n",
        "        super(myunet, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        self.resnet_features = nn.Sequential(*list(self.resnet.children())[:-2])\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 3)\n",
        "\n",
        "        self.pretrained_unet = models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
        "        self.pretrained_unet.backbone.conv1 = nn.Conv2d(512, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        # for param in self.pretrained_unet.backbone.conv1.parameters():\n",
        "        #   param.requires_grad = False\n",
        "        self.conv = nn.Conv2d(64, 1, 1)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64, 3)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(p=0.5)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet_features(x)\n",
        "\n",
        "        #x = self.pretrained_unet(x)\n",
        "\n",
        "        upconvfeatures = self.pretrained_unet.backbone.conv1(x)\n",
        "        mask  = nn.Sigmoid()(self.conv(upconvfeatures))\n",
        "        mask_rescaled = nn.functional.interpolate(mask, size=(256, 256), mode='bilinear', align_corners=False) # mask a size de 256x 256\n",
        "\n",
        "        pool = self.avgpool(upconvfeatures)\n",
        "        flatten = torch.flatten(pool, 1)\n",
        "        flatten = self.dropout1(flatten)\n",
        "        label = nn.Softmax(dim=1)(self.fc(flatten))\n",
        "\n",
        "        return mask_rescaled, label\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xznHnQpv16bz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para agilizar aun mas el proceso de entrenamiento y validacion estoy mandando mi modelo y variables a cuda"
      ],
      "metadata": {
        "id": "ULnb214QAUBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"DEVICE: {device}\")\n",
        "model = myunet()\n",
        "model = model.to(device)\n",
        "epochs = 10\n",
        "criterion_1 = nn.BCELoss()\n",
        "criterion_2 = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_accuracy = []\n",
        "    for images, masks, labels in trainloader:\n",
        "        images, masks, labels = images.to(device), masks.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        mask_out, label_out = model(images)\n",
        "        loss = criterion_1(mask_out, masks) + criterion_2(label_out, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        _, predicted_labels = torch.max(label_out, 1)\n",
        "        accuracy = (predicted_labels == labels).float().mean()\n",
        "        train_accuracy.append(accuracy)\n",
        "\n",
        "    val_loss_1 = 0.0\n",
        "    val_loss_2 = 0.0\n",
        "    val_correct = 0\n",
        "    total_val = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for val_images, val_mask, val_labels in valloader:\n",
        "            val_images, val_mask, val_labels = val_images.to(device), val_mask.to(device), val_labels.to(device)\n",
        "            val_mask_out, val_label_out = model(val_images)\n",
        "            val_loss_1 += criterion_1(val_mask_out, val_mask).item()\n",
        "            val_loss_2 += criterion_2(val_label_out, val_labels).item()\n",
        "            _, predicted_labels = torch.max(val_label_out, 1)\n",
        "            val_correct += (predicted_labels == val_labels).sum().item()\n",
        "            total_val += val_labels.size(0)\n",
        "\n",
        "    val_loss = (val_loss_1 + val_loss_2) / (len(valloader) * 2)\n",
        "    val_accuracy = val_correct / total_val\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Train Accuracy: {sum(train_accuracy) / len(train_accuracy)}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}')\n",
        "\n",
        "# Test\n",
        "model.eval()\n",
        "test_accuracy = []\n",
        "with torch.no_grad():\n",
        "    for test_images, test_mask, test_labels in testloader:\n",
        "        test_images, test_mask, test_labels = test_images.to(device), test_mask.to(device), test_labels.to(device)\n",
        "        _, test_label_out = model(test_images)\n",
        "        _, predicted_labels = torch.max(test_label_out, 1)\n",
        "        test_accuracy.append((predicted_labels == test_labels).float().mean())\n",
        "\n",
        "print(f'Test Accuracy: {sum(test_accuracy) / len(test_accuracy)}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGTKoiJWKtUz",
        "outputId": "a50343ab-8338-4e26-d117-0a178ad0d409"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEVICE: cuda\n",
            "Epoch 1, Train Accuracy: 0.5315837264060974, Val Loss: 0.679998889510482, Val Accuracy: 0.6075318441942035\n",
            "Epoch 2, Train Accuracy: 0.4541586637496948, Val Loss: 1.2682648898820597, Val Accuracy: 0.37013106885730107\n",
            "Epoch 3, Train Accuracy: 0.4376460611820221, Val Loss: 0.7610635900959317, Val Accuracy: 0.5172604762783829\n",
            "Epoch 4, Train Accuracy: 0.5262892842292786, Val Loss: 0.6912172655557794, Val Accuracy: 0.6016245154144361\n",
            "Epoch 5, Train Accuracy: 0.6122164130210876, Val Loss: 0.6295172279061426, Val Accuracy: 0.6773121654052058\n",
            "Epoch 6, Train Accuracy: 0.6531312465667725, Val Loss: 0.6306127746386722, Val Accuracy: 0.6697434004061289\n",
            "Epoch 7, Train Accuracy: 0.6619862914085388, Val Loss: 0.6306657191242239, Val Accuracy: 0.6845117223555474\n",
            "Epoch 8, Train Accuracy: 0.6947054862976074, Val Loss: 0.5842614591781503, Val Accuracy: 0.757614916005169\n",
            "Epoch 9, Train Accuracy: 0.7079492211341858, Val Loss: 0.754668644048631, Val Accuracy: 0.5578733616392837\n",
            "Epoch 10, Train Accuracy: 0.6323060393333435, Val Loss: 0.6457815628320088, Val Accuracy: 0.7053719771091009\n",
            "Test Accuracy: 0.7074262499809265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La siguiente celda es usada para guardar el modelo"
      ],
      "metadata": {
        "id": "72Ydjq1rAih6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = 'model_unet_and_resnet.pth'\n",
        "torch.save(model.state_dict(), model_save_path)\n"
      ],
      "metadata": {
        "id": "OydsslPlO9bt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JGy_j1brS2VY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}